<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="PyPi,爬虫,scrapy," />










<meta name="description" content="一. Scrapy 架构组件   P13 组件 描述 类型     ENGINE 引擎,    SCHEDULER 调度器,    DOWNLOADER 下载器,    SPIDER 爬虫,    MIDDLEWARE 中间件,    ITEM PIPELINE 数据管道,    框架中的数据流    P14 对象 描述     REQUEST Scrapy 中的 HTTP 请求对象   RES">
<meta name="keywords" content="PyPi,爬虫,scrapy">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy">
<meta property="og:url" content="http://www.pyfdtic.com/2018/03/15/crawler-Scrapy/index.html">
<meta property="og:site_name" content="Pyfdtic&#39;s Blog">
<meta property="og:description" content="一. Scrapy 架构组件   P13 组件 描述 类型     ENGINE 引擎,    SCHEDULER 调度器,    DOWNLOADER 下载器,    SPIDER 爬虫,    MIDDLEWARE 中间件,    ITEM PIPELINE 数据管道,    框架中的数据流    P14 对象 描述     REQUEST Scrapy 中的 HTTP 请求对象   RES">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://www.pyfdtic.com/imgs/scrapy/scrapy_architecture_02.png">
<meta property="og:updated_time" content="2018-04-30T01:02:03.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scrapy">
<meta name="twitter:description" content="一. Scrapy 架构组件   P13 组件 描述 类型     ENGINE 引擎,    SCHEDULER 调度器,    DOWNLOADER 下载器,    SPIDER 爬虫,    MIDDLEWARE 中间件,    ITEM PIPELINE 数据管道,    框架中的数据流    P14 对象 描述     REQUEST Scrapy 中的 HTTP 请求对象   RES">
<meta name="twitter:image" content="http://www.pyfdtic.com/imgs/scrapy/scrapy_architecture_02.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.pyfdtic.com/2018/03/15/crawler-Scrapy/"/>





  <title>Scrapy | Pyfdtic's Blog</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-115793075-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Pyfdtic's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">但行好事, 莫问前程.</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.pyfdtic.com/2018/03/15/crawler-Scrapy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pyfdtic">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pyfdtic's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Scrapy</h2>
        

        <div class="post-meta">
	  
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-15T16:45:24+08:00">
                2018-03-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="/imgs/scrapy/scrapy_architecture_02.png" alt="Scrapy 架构图"></p>
<h2 id="一-Scrapy-架构组件"><a href="#一-Scrapy-架构组件" class="headerlink" title="一. Scrapy 架构组件"></a>一. Scrapy 架构组件</h2><table>
<thead>
<tr>
<th>P13 组件</th>
<th>描述</th>
<th>类型</th>
</tr>
</thead>
<tbody>
<tr>
<td>ENGINE</td>
<td>引擎,</td>
<td></td>
</tr>
<tr>
<td>SCHEDULER</td>
<td>调度器,</td>
<td></td>
</tr>
<tr>
<td>DOWNLOADER</td>
<td>下载器,</td>
<td></td>
</tr>
<tr>
<td>SPIDER</td>
<td>爬虫,</td>
<td></td>
</tr>
<tr>
<td>MIDDLEWARE</td>
<td>中间件,</td>
<td></td>
</tr>
<tr>
<td>ITEM PIPELINE</td>
<td>数据管道,</td>
</tr>
</tbody>
</table>
<p>框架中的数据流</p>
<table>
<thead>
<tr>
<th>P14 对象</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>REQUEST</td>
<td>Scrapy 中的 HTTP 请求对象</td>
</tr>
<tr>
<td>RESPONSE</td>
<td>Scrapy 中的 HTTP 响应对象</td>
</tr>
<tr>
<td>ITEM</td>
<td>从页面中爬去的一项数据</td>
</tr>
</tbody>
</table>
<p><code>Request(url[, callback, method=&#39;GET&#39;, headers, body, cookies, meta, encoding=&#39;utf-8&#39;, priority=0, dont_filter=False, errback])</code></p>
<p>Response 对象</p>
<ol>
<li><code>HtmlResponse</code></li>
<li><code>TextResponse</code></li>
<li><code>XmlResponse</code></li>
</ol>
<h2 id="二-Spider"><a href="#二-Spider" class="headerlink" title="二. Spider"></a>二. Spider</h2><p><code>start_urls.parse / start_requests.callback --&gt; REQUEST --&gt; DOWNLOADER --&gt; RESPONSE --&gt; Selector/SelectorList[xpath/css][extract/re/extract_first/re_first]</code></p>
<h2 id="三-Selector-提取数据"><a href="#三-Selector-提取数据" class="headerlink" title="三. Selector 提取数据"></a>三. Selector 提取数据</h2><ol>
<li><p>构造 Selector</p>
<pre><code>html = &quot;
    &lt;html lang=&quot;en&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;title&gt;Title&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;h1&gt;Hello world!&lt;/h1&gt;
        &lt;p&gt;This is a line.&lt;/p&gt;
    &lt;/body&gt;
    &lt;/html&gt;&quot;
</code></pre></li>
</ol>
<pre><code># 使用文本字符串构造
from scrapy.selector import Selector
selector = Selector(text=html)

# 使用 Response 对象构造
from scrapy.http import HtmlResponse
response = HtmlResponse(url=&quot;http://www.example.com&quot;, body=body, encoding=&apos;utf-8&apos;)
selector = Selector(response=response)
</code></pre><ol>
<li><p>RESPONSE 内置 selector : 在第一次访问一个 Response 对象的 selector 属性时, Response 对象内部会以自身为参数, 自动创建 Selector 对象, 并将该 Selector 对象缓存, 以便下次使用.</p>
<pre><code>class TextResponse(Response):
    def __init__(self, *args, **kwargs):
        ...
        self._cached_selector = None
        ...

    @property
    def selector(self):
        from scrapy.selector import Selector
        if self._cached_selector is None:
            self._cached_selector = Selector(self)
        return self._cached_selector

    ...

    def xpath(self, query, **kwargs):
        return self.selector.xpath(query, **kwargs)

    def css(self, query):
        return self.selector.css(query)
</code></pre></li>
</ol>
<ol>
<li><p>选中数据 <code>XPATH/CSS</code></p>
</li>
<li><p>提取数据 <code>extract()/re()/extract_first()/re_first()</code></p>
</li>
</ol>
<h2 id="四-Item-封装数据"><a href="#四-Item-封装数据" class="headerlink" title="四. Item 封装数据"></a>四. Item 封装数据</h2><p>Scrapy 提供一下两个类, 用户可以使用它们自定义数据类(如书籍信息), 封装爬取到的数据.</p>
<ul>
<li><code>Item 基类</code> : 自定义数据类的基类, 支持字典接口(访问 自定义数据类 中的字段与访问字典类似, 支持 get() 方法).</li>
<li><code>Field 类</code>  : 用来描述自定义数据类包含哪些字段(如name, price 等).</li>
</ul>
<h3 id="1-自定义数据类-只需继承-Item-并创建一系列-Field-对象的类属性即可-类似于-ORM-创建的-Model"><a href="#1-自定义数据类-只需继承-Item-并创建一系列-Field-对象的类属性即可-类似于-ORM-创建的-Model" class="headerlink" title="1. 自定义数据类, 只需继承 Item, 并创建一系列 Field 对象的类属性即可, 类似于 ORM 创建的 Model."></a>1. 自定义数据类, 只需继承 Item, 并创建一系列 Field 对象的类属性即可, 类似于 ORM 创建的 Model.</h3><pre><code>from scrapy import Item, Field


class BookItem(Item):
    name = Field()
    price = Field()


class ForeignBookItem(BookItem):
    &quot;&quot;&quot;
    扩展 BookItem 类
    &quot;&quot;&quot;
    translator = Field()
</code></pre><h3 id="2-Field-元数据"><a href="#2-Field-元数据" class="headerlink" title="2. Field 元数据"></a>2. Field 元数据</h3><pre><code>class BookItem(Item):
    name = Field(a=123, b=[1,2,3])
    price = Field(a=lambda x: x+2)

# Field 是 Python 字典的子类, 可以通过键获取 Field 对象中的元数据.

b = BookItem(name=100, price=101)
b[&apos;name&apos;]   # 100
b[&apos;price&apos;]  # 101
b.fields    # {&apos;name&apos;: {&apos;a&apos;: 123, &apos;b&apos;: [1, 2, 3]}, &apos;price&apos;: {&apos;a&apos;: &lt;function __main__.&lt;lambda&gt;&gt;}}
</code></pre><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><pre><code>class BookItem(Item):
    ...
    # 当 authors 是一个列表而不是一个字符串时, 串行化为一个字符串.
    authors = Field(serializer=lambda x: &quot;|&quot;.join(x))
    ...
</code></pre><p>以上例子中, 元数据键 serializer 时 CSVItemExportr 规定好的, 他会用该键获取元数据, 即一个串行化函数对象, 并使用这个串行化函数将 authors 字符串行化成一个字符串, 具体代码参考 <code>scrapy/exporters.py</code> 文件. </p>
<h2 id="五-Item-Pipeline-处理数据"><a href="#五-Item-Pipeline-处理数据" class="headerlink" title="五. Item Pipeline 处理数据"></a>五. Item Pipeline 处理数据</h2><p>在 Scrapy 中, Item Pipeline 是处理数据的组件, 一个 Item Pipeline 就是一个包含特定接口的类, 通常只负责一种功能的数据处理, 在一个项目中可以同时启动多个 Item Pipeline, 他们按指定次序级联起来, 形成一条数据处理流水线.</p>
<p>Item Pipeline 典型应用</p>
<ul>
<li>数据请求</li>
<li>验证数据有效性</li>
<li>过滤重复数据</li>
<li>将数据存入数据库.</li>
</ul>
<h3 id="1-编写-pipeline"><a href="#1-编写-pipeline" class="headerlink" title="1. 编写 pipeline"></a>1. 编写 pipeline</h3><pre><code>class PriceConverterPipeline(object):

    # 英镑兑换人民币汇率
    exchange_rate = 8.5309

    def process_item(self, item, spider):
        # 提取 item 的 price 字段(如 £ 53.74), 去掉 £ 符号, 转换为 float 类型, 乘以汇率
        price = float(item[&quot;price&quot;][1:]) * self.exchange_rate

        # 保留两位小数赋值,
        item[&quot;price&quot;] = &quot;¥ %.2f&quot; % price

        return item
</code></pre><p>一个 Item Pipeline 不需要继承特定基类, 只需要实现某些特定的方法.</p>
<ul>
<li><p><code>process_item(self, item, spider)</code> : 该方法必须实现.<br>  该方法用来处理每一项由 Spider 爬取到的数据, 其中 item 为爬取到的一项数据, Spider 为爬取此项数据的 Spider 对象.</p>
<p>  <code>process_item</code> 是 Item Pipeline 的核心, process_item 返回的一项数据, 会传递给下一级 Item Pipeline(如果有) 继续处理.</p>
<p>  如果 process_item 在处理某项 item 时抛出 DropItem(scrapy.exceptions.DropItem) 异常, 该项 item 便会被抛弃, 不会传递到下一级 Item Pipeline, 也不会导出到文件. 通常, 在检测到无效数据, 或者希望过滤的数据时, 抛出该异常.</p>
</li>
<li><p><code>open_spider(self, spider)</code></p>
<p>  Spider 打开时(处理数据前), 回调该方法, 通常该方法用于在 开始处理数据之前完成某些<strong>初始化</strong>的工作, 如连接数据库.</p>
</li>
<li><p><code>close_spider(self, spider)</code></p>
<p>  Spider 关闭时(处理数据后), 回调该方法, 通常该方法用于在 处理完所有数据之后完成某些<strong>清理</strong>工作, 如关闭数据库链接.</p>
</li>
<li><p><code>from_crawler(cls, crawler)</code></p>
<p>  创建 Item Pipeline 对象时回调该类方法. 通常, 在该方法中通过 crawler.settings 读取配置, 根据配置创建 Item Pipeline 对象.</p>
<p>  如果一个 Item Pipeline 定义了 from_crawler 方法, Scrapy 就会调用该方法来创建 Item Pipeline 对象. 该方法的两个参数:</p>
<ul>
<li>cls : Item Pipeline 类的对象,</li>
<li>crawler : crawler 是 Scrapy 中的一个核心对象, 可以通过 crawler.settings 属性访问配置文件.</li>
</ul>
</li>
</ul>
<h3 id="2-启用-Item-Pipeline"><a href="#2-启用-Item-Pipeline" class="headerlink" title="2. 启用 Item Pipeline."></a>2. 启用 Item Pipeline.</h3><pre><code>$ cat settings.py

    ITEM_PIPELINES = {
       &apos;example.pipelines.PriceConverterPipeline&apos;: 300,
    }
</code></pre><p>ITEM_PIPELINES 是一个字典, 其中每一项 Item Pipeline 类的导入路径, 值是一个 0~1000 的数字, 同时启用多个 Item Pipeline 时, Scrapy 根据这些数值决定各 Item Pipeline 处理数据的先后次序, 数值小的优先级高.</p>
<h3 id="3-代码示例"><a href="#3-代码示例" class="headerlink" title="3. 代码示例"></a>3. 代码示例</h3><h4 id="3-1-Item-Pipeline-过滤重复数据"><a href="#3-1-Item-Pipeline-过滤重复数据" class="headerlink" title="3.1 Item Pipeline : 过滤重复数据"></a>3.1 Item Pipeline : 过滤重复数据</h4><pre><code>class DuplicatesPipeline(object):
    def __init__(self):
        self.book_set = set()

    def process_item(self, item, spider):
        name = item[&quot;name&quot;]
        if name in self.book_set:
            raise DropItem(&quot;Duplicate book found: %s&quot; % item)

        self.book_set.add(name)
        return item
</code></pre><h4 id="3-2-Item-Pipeline-将数据存储-MongoDB"><a href="#3-2-Item-Pipeline-将数据存储-MongoDB" class="headerlink" title="3.2 Item Pipeline : 将数据存储 MongoDB"></a>3.2 Item Pipeline : 将数据存储 MongoDB</h4><pre><code># pipelines.py
class MongoDBPipeline(object):

    @classmethod
    def from_crawler(cls, crawler):
        &quot;&quot;&quot;
        使用 settings.py 配置文件, 配置 MongoDB 数据库, 而不是硬编码
        如果一个 Item Pipeline 定义了 from_crawler 方法,
        Scrapy 就会调用该方法来创建 Item Pipeline 对象.
        &quot;&quot;&quot;
        cls.DB_URI = crawler.settings.get(&quot;MONGO_DB_URI&quot;,
                                          &quot;mongodb://localhost:27017/&quot;)

        cls.DB_NAME = crawler.settings.get(&quot;MONGO_DB_NBAME&quot;, &quot;scrapy_data&quot;)

        return cls()

    def open_spider(self, spider):
        &quot;&quot;&quot;在开始处理数据之前, 链接数据库&quot;&quot;&quot;
        self.client = pymongo.MongoClient(self.DB_URI)
        self.db = self.client[self.DB_NAME]

    def close_spider(self, spider):
        &quot;&quot;&quot;数据处理完成之后, 关闭数据库链接&quot;&quot;&quot;
        self.client.close()

    def process_item(self, item, spider):
        &quot;&quot;&quot;将 item 数据 写入 MongoDB&quot;&quot;&quot;
        collection = self.db[spider.name]
        post = dict(item) if isinstance(item, Item) else item
        collection.insert_one(post)

        return item

# settings.py
MONGO_DB_URI = &quot;mongodb://192.168.1.1:27017/&quot;
MONGO_DB_NBAME = &quot;my_scrapy_data&quot;

ITEM_PIPELINES = {
    &quot;toscrapt_book.pipelines.MongoDBPipeline&quot;: 403,
}
</code></pre><h4 id="3-3-Item-Pipeline-将数据存储-Mysql"><a href="#3-3-Item-Pipeline-将数据存储-Mysql" class="headerlink" title="3.3 Item Pipeline : 将数据存储 Mysql"></a>3.3 Item Pipeline : 将数据存储 Mysql</h4><p>版本一 : </p>
<pre><code># pipelines.py
class MySQLPipeline(object):
    def open_spider(self, spider):
        db = spider.settings.get(&quot;MYSQL_DB_NAME&quot;, &quot;scrapy_data&quot;)
        host = spider.settings.get(&quot;MYSQL_HOST&quot;, &quot;locahost&quot;)
        port = spider.settings.get(&quot;MYSQL_PORT&quot;, &quot;3306&quot;)
        user = spider.settings.get(&quot;MYSQL_USER&quot;, &quot;root&quot;)
        password = spider.settings.get(&quot;MYSQL_PASSWORD&quot;, &quot;123456&quot;)

        self.db_conn = MySQLdb.connect(host=host, port=port, db=db, user=user, passwd=password, charset=&quot;utf-8&quot;)
        self.db_cur = self.db_conn.cursor()

    def closer_spider(self, spider):
        self.db_conn.commit()
        self.db_conn.close()

    def procecss_item(self, item, spider):
        self.insert_db(item)

        return item

    def insert_db(self, item):
        values = (
            item[&quot;name&quot;],
            item[&quot;price&quot;],
            item[&quot;rating&quot;],
            item[&quot;number&quot;]
        )

        sql = &quot;INSERT INTO books VALUES (%s, %s, %s, %s, )&quot;
        self.db_cur.execute(sql, values)


# settings.py
MYSQL_DB_NAME = &quot;scrapy_data&quot;
MYSQL_HOST = &quot;locahost&quot;
MYSQL_PORT = &quot;3306&quot;
MYSQL_USER = &quot;root&quot;
MYSQL_PASSWORD = &quot;123456&quot;

ITEM_PIPELINES = {
    &quot;toscrapt_book.pipelines.MySQLPipeline&quot;: 403,
}
</code></pre><p>版本二: Scrapy 框架本身使用 Twisted 编写, Twisted 是一个事件驱动型的异步网络框架, 鼓励用户编写异步代码, Twisted 中提供了以异步方式多线程访问数据库的模块 <code>adbapi</code>, 使用该模块可以显著提高程序访问数据库的效率.</p>
<pre><code># pipelines.py
from twisted.enterprise import adbapi

class MySQLAsyncPipeline(object):
    def open_spider(self, spider):
        db = spider.settings.get(&quot;MYSQL_DB_NAME&quot;, &quot;scrapy_data&quot;)
        host = spider.settings.get(&quot;MYSQL_HOST&quot;, &quot;locahost&quot;)
        port = spider.settings.get(&quot;MYSQL_PORT&quot;, &quot;3306&quot;)
        user = spider.settings.get(&quot;MYSQL_USER&quot;, &quot;root&quot;)
        password = spider.settings.get(&quot;MYSQL_PASSWORD&quot;, &quot;123456&quot;)

        # adbapi.ConnectionPool 可以创建一个数据库连接池对象, 其中包含多个链接对象, 每个链接对象在单独的线程中工作.
        # adbapi 只提供异步访问数据库的框架, 其内部依然使用 MySQLdb, sqlite3 这样的库访问数据库.
        self.dbpool = adbapi.ConnectionPool(&quot;MySQLdb&quot;, host=host, database=db,
                                            user=user, password=password, charset=&quot;utf-8&quot;)

    def close_spider(self, spider):
        self.dbpool.close()

    def process_item(self, item, spider):
        # 以异步方式调用 insert_db 方法, 执行完 insert_db 方法之后, 链接对象自动调用 commit 方法.
        self.dbpool.runInteraction(self.insert_db, item)
        return item

    def insert_db(self, tx, item):
        # tx 是一个 Transaction 对象, 其接口与 Cursor 对象类似, 可以调用 execute 执行 SQL 语句.
        values = (
            item[&quot;name&quot;],
            item[&quot;price&quot;],
            item[&quot;rating&quot;],
            item[&quot;number&quot;]
        )

        sql = &quot;INSERT INTO books VALUES (%s, %s, %s, %s, )&quot;
        tx.execute(sql, values)

# settings.py
MYSQL_DB_NAME = &quot;scrapy_data&quot;
MYSQL_HOST = &quot;locahost&quot;
MYSQL_PORT = &quot;3306&quot;
MYSQL_USER = &quot;root&quot;
MYSQL_PASSWORD = &quot;123456&quot;

ITEM_PIPELINES = {
    &quot;toscrapt_book.pipelines.MySQLAsyncPipeline&quot;: 403,
}
</code></pre><h4 id="3-4-Item-Pipeline-将数据存储-Redis"><a href="#3-4-Item-Pipeline-将数据存储-Redis" class="headerlink" title="3.4 Item Pipeline : 将数据存储 Redis"></a>3.4 Item Pipeline : 将数据存储 Redis</h4><pre><code># pipelines.py
import redis
from scrapy import Item

class RedisPipeline(object):
    def open_spider(self, spider):
        db_host = spider.settings.get(&quot;REDIS_HOST&quot;, &quot;localhost&quot;)
        db_port = spider.settings.get(&quot;REDIS_PORT&quot;, &quot;6379&quot;)
        db_index = spider.settings.get(&quot;REDIS_DB_INDEX&quot;, 0)

        self.db_conn = redis.StrictRedis(host=db_host, port=db_port, db=db_index)
        self.item_i = 0

    def close_spider(self, spider):
        self.db_conn.connection_pool.disconnect()

    def process_item(self, item, spider):
        self.insert_db(item)

        return item

    def insert_db(self, item):
        if isinstance(item, Item):
            item = dict(item)

        self.item_i += 1
        self.db_conn.hmset(&quot;book:%s&quot; % self.item_i, item)


# settings.py
&quot;REDIS_HOST&quot; = &quot;localhost&quot;
&quot;REDIS_PORT&quot; = 6379
&quot;REDIS_DB_INDEX&quot; = 0

ITEM_PIPELINES = {
    &quot;toscrapt_book.pipelines.RedisPipeline&quot;: 403,
}    
</code></pre><h2 id="六-LinkExtractor-提取链接"><a href="#六-LinkExtractor-提取链接" class="headerlink" title="六. LinkExtractor 提取链接"></a>六. LinkExtractor 提取链接</h2><p>提取页面中的链接, 有两种方法:</p>
<ul>
<li><code>selector</code> : 提取少量链接时, 或提取规则比较简单.</li>
<li><code>LinkExtractor</code> : 专门用于提取链接的类 LinkExtractor.</li>
</ul>
<h3 id="1-使用示例"><a href="#1-使用示例" class="headerlink" title="1. 使用示例:"></a>1. 使用示例:</h3><pre><code># url from selector
# next_url = response.css(&quot;ul.pager li.next a::attr(href)&quot;).extract_first()
# if next_url:
#     next_url = response.urljoin(next_url)
#     yield scrapy.Request(next_url, callback=self.parse)

# url from LinkeExtractor
le = LinkExtractor(restrict_css=&quot;ul.pager li.next&quot;)
links = le.extract_links(response)      # 返回一个列表, 其中每一个元素都是一个 Link 对象, Link 对象的 url 属性便是链接页面的 绝对 url 地址.

if links:
    next_url = links[0].url
    yield scrapy.Request(next_url, callback=self.parse)
</code></pre><h3 id="2-链接提取规则"><a href="#2-链接提取规则" class="headerlink" title="2. 链接提取规则:"></a>2. 链接提取规则:</h3><p>LinkExtractor 构造器的所有参数都有默认值, 如果构造对象时, 不传递任何参数(使用默认值), 则提取页面中的所有链接.</p>
<ul>
<li><p><code>allow</code> : 接受一个正则表达式或一个正则表达式列表, 提取绝对 url 与 正则表达式匹配的链接, 如果该参数为空(默认), 则提取全部链接.</p>
<pre><code>&gt;&gt;&gt; pattern = &apos;/intro/.+\.html$&apos;
&gt;&gt;&gt; le = LinkExtractor(allow=pattern)
&gt;&gt;&gt; links = le.extract_links(response)
</code></pre></li>
<li><p><code>deny</code> : 接受一个正则表达式或一个正则表达式列表,  与 allow 相反, 排除绝对 url 与正则表示匹配的链接.</p>
<pre><code>&gt;&gt;&gt; pattern = &apos;^http://example.com&apos;
&gt;&gt;&gt; le = LinkExtractor(deny=pattern)
&gt;&gt;&gt; links = le.extract_links(response)
</code></pre></li>
<li><p><code>allow_domains</code> : 接受一个域名或一个域名列表, 提取到指定域的链接.</p>
<pre><code>&gt;&gt;&gt; domains = [&quot;github.com&quot;, &quot;stackoverflow.com&quot;]
&gt;&gt;&gt; le = LinkExtractor(allow_domains=domains)
&gt;&gt;&gt; links = le.extract_links(response)
</code></pre></li>
<li><p><code>deny_domains</code> :  接受一个域名或一个域名列表, 与 allow_domains 相反, 排除到指定域的链接.</p>
<pre><code>&gt;&gt;&gt; domains = [&quot;github.com&quot;, &quot;stackoverflow.com&quot;]
&gt;&gt;&gt; le = LinkExtractor(deny_domains=domains)
&gt;&gt;&gt; links = le.extract_links(response)
</code></pre></li>
<li><p><code>restrict_xpaths</code> : 接受一个 Xpath 表达式或一个 Xpath 表达式列表, 提取 XPath 表达式选中区域下的的链接.</p>
<pre><code>&gt;&gt;&gt; le = LinkExtractor(restrict_xpaths=&quot;//div[@id=&apos;top&apos;]&quot;)
&gt;&gt;&gt; links = le.extract_links(response)
</code></pre></li>
<li><p><code>restrict_css</code> : 接受一个 CSS 选择器或一个 CSS 选择器列表, 提取 CSS 选择器选中区域下的的链接.</p>
<pre><code>&gt;&gt;&gt; le = LinkExtractor(restrict_css=&quot;div#bottom&quot;)
&gt;&gt;&gt; links = le.extract_links(response)
</code></pre></li>
<li><p><code>tags</code> : 接受一个标签(字符串)或一个标签列表, 提取指定标签内的链接, 默认为 <code>[&quot;a&quot;, &quot;area&quot;]</code></p>
</li>
<li><p><code>attrs</code> : 接受一个属性(字符串)或一个属性列表, 提取指定属性内的链接, 默认为 <code>[&quot;href&quot;]</code></p>
<pre><code># &lt;script type=&quot;text/javascript&quot; src=&quot;/js/app.js&quot; /&gt;

&gt;&gt;&gt; le = LinkExtractor(tags=&quot;script&quot;, attrs=&quot;src&quot;)
&gt;&gt;&gt; links = le.extract_links(response)
</code></pre></li>
<li><p><code>process_value</code> : 接受一个形如 <code>func(value)</code> 的回调函数. 如果传递了该参数, LinkExtractor 将调用该回调函数对提取的每一个链接(如 a 的 href) 进行处理, 回调函数正常情况下应该返回一个字符串(处理结果), 想要抛弃所处理的连接时, 返回 None.</p>
<pre><code># &lt;a href=&quot;javascript:goToPage(&apos;/doc.html&apos;); return false&quot;&gt;文档&lt;/a&gt;

&gt;&gt;&gt; import re
&gt;&gt;&gt; def process(value):
        m = re.search(&quot;javascript:goToPage\(&apos;(.*?)&apos;&quot;, value)
        # 如果匹配, 就提取其中 url 并返回, 不匹配则返回原值.
        if m:
            value = m.group()
        return value

&gt;&gt;&gt; le = LinkExtractor(process_value=process)
&gt;&gt;&gt; links = le.extract_links(response)      
</code></pre></li>
</ul>
<h2 id="七-Exporter-导出数据"><a href="#七-Exporter-导出数据" class="headerlink" title="七. Exporter 导出数据"></a>七. Exporter 导出数据</h2><p>在 Scrapy 中, 负责导出数据的组件被称为 Exporter(导出器), Scrapy 内部实现了多个 Exporter , 每个 Exporter 实现一种数据格式的导出. 支持的数据导出格式如下(括号内为对应的  Exporter):</p>
<ul>
<li>JSON (JsonItemExporter)</li>
<li>JSON lines (JsonLinesItemExporter)</li>
<li>CSV (CsvItemExporter)</li>
<li>XML (XmlItemExporter)</li>
<li>Pickle (PickleItemExporter)</li>
<li>Marshal (MarshalItemExporter)</li>
</ul>
<p>Scrapy 爬虫会议 <code>-t</code> 参数中的数据格式字符串(如 csv, json, xml) 为键, 在配置字典 <code>FEED_EXPORTERS</code> 中搜索 Exporter, <code>FEED_EXPORTERS</code> 的内容由以下两个字典的内容合并而成:</p>
<ul>
<li>默认配置文件中的 <code>FEED_EXPORTERS_BASE</code>, 为 Scrapy 内部支持的导出数据格式, 位于 <code>scrapy.settings.default_settings</code></li>
<li><p>用户配置文件中的 <code>FEED_EXPORTERS</code>, 为用户自定义的导出数据格式, 在配置文件 <code>settings.py</code> 中.</p>
<pre><code>FEED_EXPORTERS = {&quot;excel&quot;: &quot;my_propject.my_exporters.ExcelItemExporter&quot;}
</code></pre></li>
</ul>
<h3 id="1-导出数据方式"><a href="#1-导出数据方式" class="headerlink" title="1. 导出数据方式:"></a>1. 导出数据方式:</h3><ol>
<li><p>命令行参数</p>
<pre><code>$ scrapy crawl CRAWLER -t FORMAT -o /path/to/save.file 
$ scrapy crawl books -t csv -o books.data
$ scrapy crawl books -t xml -o books.data
$ scrapy crawl books -t json -o books.data

$ scrapy craw books -o books.csv    # Scrapy 可以通过文件后缀名, 推断出文件格式, 从而省去 -t 参数. 如 `-o books.json`
</code></pre><p> <code>-o /path/to/file</code> 支持变量: 如 <code>scrapy crawl books -o &#39;export_data/%(name)s/%(time)s.csv&#39;</code></p>
<ul>
<li><code>%(name)s</code> : Spider 的名字</li>
<li><code>%(time)s</code> : 文件创建时间</li>
</ul>
</li>
<li><p>通过配置文件指定.</p>
<p> 常用选项如下:</p>
<ul>
<li><p><code>FEED_URL</code> : 导出文件路径</p>
<pre><code>FEED_URL = &apos;export_data/%(name)s/%(csv)s.data&apos;
</code></pre></li>
<li><p><code>FEED_FORMAT</code> : 导出数据格式</p>
<pre><code>FEED_FORMAT = &apos;csv&apos;
</code></pre></li>
<li><p><code>FEED_EXPORT_ENCODING</code> : 导出文件编码, 默认 json 使用数字编码, 其他使用 utf-8 编码</p>
<pre><code>FEED_EXPORT_ENCODING = &quot;gbk&quot;
</code></pre></li>
<li><p><code>FEED_EXPORT_FIELDS</code> : 导出数据包含的字段(默认情况下, 导出所有字段), 并指定导出顺序.</p>
<pre><code>FEED_EXPORT_FIELDS = [&quot;name&quot;, &quot;author&quot;, &quot;price&quot;]
</code></pre></li>
<li><p><code>FEED_EXPORTERS</code> : 用户自定义的 Exporter 字典, 添加新的导出数据格式时使用.</p>
<pre><code>FEED_EXPORTERS = {&quot;excel&quot;: &quot;my_project.my_exporters.ExcelItemExporter&quot;}
</code></pre></li>
</ul>
</li>
</ol>
<h3 id="2-自定义数据导出格式"><a href="#2-自定义数据导出格式" class="headerlink" title="2. 自定义数据导出格式"></a>2. 自定义数据导出格式</h3><pre><code>import six
from scrapy.utils.serialize import ScrapyJSONEncoder
import xlwt


class BaseItemExporter(object):
    def __init__(self, **kwargs):
        self._configure(kwargs)

    def _configure(self, options, dont_fail=False):
        self.encoding = options.pop(&quot;encoding&quot;, None)
        self.fields_to_export = options.pop(&quot;field_to_export&quot;, None)
        self.export_empty_fields = options.pop(&quot;export_empty_fields&quot;, False)

        if not dont_fail and options:
            raise TypeError(&quot;Unexpected options: %s&quot; % &apos;,&apos;.join(options.keys()))

    def export_item(self, item):
        &quot;&quot;&quot;
        负责导出爬去到的每一项数据, 参数 item 为一项爬取到的数据, 每个子类必须实现该方法.
        :param item:
        :return:
        &quot;&quot;&quot;
        raise NotImplementedError

    def serialize_field(self, field, name, value):
        serializer = field.get(&quot;serializer&quot;, lambda x: x)
        return serializer(value)

    def start_exporting(self):
        &quot;&quot;&quot;
        在导出开始时被调用, 可在该方法中执行某些初始化操作.
        :return:
        &quot;&quot;&quot;
        pass

    def finish_exporting(self):
        &quot;&quot;&quot;
        在导出完成时被调用, 可在该方法中执行某些清理工作.
        :return:
        &quot;&quot;&quot;
        pass

    def _get_serialized_field(self, item, default_value=None, include_empty=None):
        &quot;&quot;&quot;
        Return the fields to export as an iterable of tuples (name, serialized_value)
        :param item:
        :param default_value:
        :param include_empty:
        :return:
        &quot;&quot;&quot;
        if include_empty is None:
            include_empty = self.export_empty_fields

        if self.fields_to_export is None:
            if include_empty and not isinstance(item, dict):
                field_iter = six.iterkeys(item.fields)
            else:
                field_iter = six.iterkeys(item)
        else:
            if include_empty:
                field_iter = self.fields_to_export
            else:
                field_iter = (x for x in self.fields_to_export if x in item)

        for field_name in field_iter:
            if field_name in item:
                field = {} if isinstance(item, dict) else item.fields[field_name]
                value = self.serialize_field(field, field_name, item[field_name])
            else:
                value = default_value

            yield field_name, value


# json
class JsonItemExporter(BaseItemExporter):
    def __init__(self, file, **kwargs):
        self._configure(kwargs, dont_fail=True)
        self.file = file
        kwargs.setdefault(&quot;ensure_ascii&quot;, not self.encoding)
        self.encoder = ScrapyJSONEncoder(**kwargs)
        self.first_item = True

    def start_exporting(self):
        &quot;&quot;&quot;
        保证最终导出结果是一个 json 列表.
        :return:
        &quot;&quot;&quot;
        self.file.write(b&quot;[\n&quot;)

    def finish_exporting(self):
        &quot;&quot;&quot;
        保证最终导出结果是一个 json 列表.
        :return:
        &quot;&quot;&quot;
        self.file.write(b&quot;\n]&quot;)

    def export_item(self, item):
        &quot;&quot;&quot;
        调用 self.encoder.encode 将每一项数据转换成 json 串.
        :param item:
        :return:
        &quot;&quot;&quot;
        if self.first_item:
            self.first_item = False
        else:
            self.file.write(b&quot;,\n&quot;)

        itemdict = dict(self._get_serialized_field(item))
        data = self.encoder.encode(itemdict)
        self.file.write(to_bytes(data, self.encoding))


# 自定义 excel 导出格式.
class ExcelItemExporter(BaseItemExporter):
    def __init__(self, file, **kwargs):
        self._configure(kwargs)
        self.file = file
        self.wbook = xlwt.Workbook()
        self.wsheet = self.wbook.add_sheet(&quot;scrapy&quot;)
        self.row = 0

    def finish_exporting(self):
        self.wbook.save(self.file)

    def export_item(self, item):
        fields = self._get_serialized_field(item)   # 获取所有字段的迭代器.

        for col, v in enumerate(x for _, x in fields):
            self.wsheet.write(self.row, col, v)

        self.row += 1


$ vim settings.py

    # my_exporters.py 与 settings.py 位于同级目录下.
    FEED_EXPORTERS = {&quot;excel&quot;: &quot;example.my_exporters.ExcelItemExporter&quot;}
</code></pre><h2 id="八-下载文件-FilesPipeline-和图片-ImagesPipeline"><a href="#八-下载文件-FilesPipeline-和图片-ImagesPipeline" class="headerlink" title="八. 下载文件(FilesPipeline)和图片(ImagesPipeline)"></a>八. 下载文件(FilesPipeline)和图片(ImagesPipeline)</h2><p>FilesPipeline 和 ImagesPipeline 可以看做两个特殊的下载器, 用户使用时, 只需要铜鼓 item 的一个特殊字段将要下载文件或图片的 URL 传递给他们, 他们会自动将文件或图片下载到本地, 并将下载结果信息存入 item 的另一个特殊字段, 以便用户在导出文件中查阅.</p>
<h3 id="1-FilesPipeline-使用方法"><a href="#1-FilesPipeline-使用方法" class="headerlink" title="1. FilesPipeline 使用方法"></a>1. FilesPipeline 使用方法</h3><ol>
<li><p>在 settings.py 中启用 FilesPipeline, 通常将其置于其他 Item Pipelines 之前</p>
<pre><code>ITEM_PIPELINES = {&quot;scrapy.pipelines.files.FilesPipeline&quot;: 1}
</code></pre></li>
<li><p>在 settings.py 中使用 FILES_STORE 指定文件下载目录.</p>
<pre><code>FILES_STORE = &quot;/path/to/my/download&quot;
</code></pre></li>
<li><p>下载文件</p>
<p> 在 Spider 解析一个包含文件下载链接的也面试, 将所有需要下载文件的 url 地址收集到一个列表, 赋给 item 的 <code>file_urls</code> 字段(<code>item[&quot;file_urls&quot;]</code>). FilesPipeline 在处理每一项 item 时, 会读取 <code>item[&#39;file_urls&#39;]</code>, 对其中每一个 url 进行下载.</p>
<pre><code>class DownloadBookSpider(scrapy.Spider):
    ...
    def parse(response):
        item = {}
        item[&quot;file_urls&quot;] = []

        for url in response.xpath(&quot;//a/@href&quot;).extract():
            download_url = response.urljoin(url)
            item[&quot;file_urls&quot;].append(download_url)

        yield item
</code></pre><p> 当 FilesPipeline 下载完 <code>item[&quot;file_urls&quot;]</code> 中的所有文件后, 会将各文件的下载结果信息收集到另一个列表, 赋给 <code>item[&quot;files&quot;]</code> 字段, 下载信息结果包含以下内容:</p>
<ul>
<li><code>Path</code> : 文件下载到本地的路径, 相对于 FILES_STORE 的相对路径.</li>
<li><code>Checksum</code> : 文件的校验和</li>
<li><code>url</code> : 文件的 url 地址.</li>
</ul>
</li>
</ol>
<h4 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h4><pre><code># items.py
import scrapy

class MatplotlibFileItem(scrapy.Item):
    file_urls = scrapy.Field()
    files = scrapy.Field()  

# spiders/matplotlib.py
import scrapy
from scrapy.linkextractors import LinkExtractor

from ..items import MatplotlibFileItem


class MatplotlibSpider(scrapy.Spider):
    name = &apos;matplotlib&apos;
    allowed_domains = [&apos;matplotlib.org&apos;]
    start_urls = [&apos;https://matplotlib.org/examples/index.html&apos;]

    def parse(self, response):
        # 爬取所有 二级 页面地址
        le = LinkExtractor(restrict_css=&quot;div.toctree-wrapper.compound&quot;, deny=&quot;/index.html$&quot;)
        links = le.extract_links(response)
        for link in links:
            yield scrapy.Request(link.url, callback=self.parse_page)

    def parse_page(self, response):
        href = response.css(&quot;a.reference.external::attr(href)&quot;).extract_first()
        url = response.urljoin(href)

        example = MatplotlibFileItem()
        example[&quot;file_urls&quot;] = [url]

        yield example

# pipelines.py : 重写 FilesPipeline 的 file_path 代码, 以自定义保存路径.
from scrapy.pipelines.files import FilesPipeline
from urlparse import urlparse
from os.path import basename, dirname, join

class MyFilesPipeline(FilesPipeline):

    def file_path(self, request, response=None, info=None):
        path = urlparse(request.url).path
        return join(basename(dirname(path)), basename(path))

# settings.py
ITEM_PIPELINES = {

    # &quot;scrapy.pipelines.files.FilesPipeline&quot;: 1,
    &apos;matplotlib_file.pipelines.MyFilesPipeline&apos;: 1,
}

FILES_STORE = &quot;example_src&quot;         # 文件下载路径

# 运行:
$ scrapy crawl matplotlib -o examp.json
</code></pre><h3 id="2-ImagesPipeline"><a href="#2-ImagesPipeline" class="headerlink" title="2. ImagesPipeline"></a>2. ImagesPipeline</h3><p>图片本身也是文件, ImagesPipeline 是 FilesPipeline 的子类, 使用上和 FilesPipeline 大同小异, 只是在所使用的 item 字段和配置选项上略有差别.</p>
<table>
<thead>
<tr>
<th>Desc</th>
<th>FilesPipeline</th>
<th>ImagesPipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td>导入路径</td>
<td><code>scrapy.pipelines.files.FilesPipeline</code></td>
<td><code>scrapy.pipelines.images.ImagesPipeline</code></td>
</tr>
<tr>
<td>Item 字段</td>
<td><code>file_urls</code>, <code>files</code></td>
<td><code>image_urls</code>, <code>images</code></td>
</tr>
<tr>
<td>下载目录</td>
<td><code>FILES_STORE</code></td>
<td><code>IMAGES_STORE</code></td>
</tr>
</tbody>
</table>
<h4 id="2-1-生成缩略图"><a href="#2-1-生成缩略图" class="headerlink" title="2.1 生成缩略图"></a>2.1 生成缩略图</h4><p>在 <code>settings.py</code> 中设置 <code>IMAGES_THUMBS</code>, 他是一个字典, 每一项的值是缩略图的尺寸.</p>
<pre><code>IMAGES_THUMBS = {
    &quot;small&quot;: (50, 50),
    &quot;big&quot;: (270, 270)
}
</code></pre><p>开启该功能后, 下载一张图片时, 会在本地出现 3 张图片, 其存储路径如下:</p>
<pre><code>[IMAGES_STORE]/full/name.jpg
[IMAGES_STORE]/thumbs/small/name.jpg
[IMAGES_STORE]/thumbs/big/name.jpg
</code></pre><h4 id="2-2-过滤尺寸过小的图片"><a href="#2-2-过滤尺寸过小的图片" class="headerlink" title="2.2 过滤尺寸过小的图片"></a>2.2 过滤尺寸过小的图片</h4><p>在 <code>settings.py</code> 中设置 <code>IMAGES_MIN_WIDTH</code> 和 <code>IMAGES_MIN_HEIGHT</code></p>
<pre><code>IMAGES_MIN_WIDTH = 110
IMAGES_MIN_HEIGHT = 110
</code></pre><h5 id="示例代码-1"><a href="#示例代码-1" class="headerlink" title="示例代码"></a>示例代码</h5><pre><code># settings.py
ITEM_PIPELINES = {
    &quot;scrapy.pipelines.images.ImagesPipeline&quot;: 1,
   # &apos;so_img.pipelines.SoImgPipeline&apos;: 300,
}
IMAGES_STORE = &apos;download_images&apos;

# spider
import json
import scrapy

class ImagesSpider(scrapy.Spider):
    IMG_TYPE = &quot;wallpaper&quot;
    IMG_START = 0
    BASE_URL = &quot;http://image.so.com/zj?ch=%s&amp;sn=%s&amp;listtype=new&amp;temp=1&quot;

    name = &apos;wallpaper&apos;
    # allowed_domains = [&apos;image.so.com&apos;]
    start_urls = [BASE_URL % (IMG_TYPE, IMG_START)]

    MAX_DOWNLOAD_NUM = 100
    start_index = 0

    def parse(self, response):
        infos = json.loads(response.body.decode(&quot;utf-8&quot;))
        yield {&quot;image_urls&quot;: [info[&quot;qhimg_url&quot;] for info in infos[&quot;list&quot;]]}

        # 如果 count 字段大于 0, 并且下载数量不足 MAX_DOWNLOAD_NUM, 继续获取下一页信息.

        self.start_index += infos[&quot;count&quot;]
        if infos[&quot;count&quot;] &gt; 0 and self.start_index &lt; self.MAX_DOWNLOAD_NUM:
            yield scrapy.Request(self.BASE_URL % (self.IMG_TYPE, self.start_index))

# 爬取图片
$ scrapy crawl wallpaper
</code></pre><h2 id="九-模拟登陆"><a href="#九-模拟登陆" class="headerlink" title="九. 模拟登陆"></a>九. 模拟登陆</h2><p>Scrapy 提供一个 <code>FormRequest 类(Request 的子类)</code>, 专门用于构造含有表单数据的请求, FormRequest 的构造器方法有一个 formdata 参数, 接受字典形式的表单数据.</p>
<ol>
<li><p>直接构造 FormRequest </p>
<pre><code>$ scrapy shell http://example.webscraping.com/places/default/user/login

&gt;&gt;&gt; sel = response.xpath(&apos;//div[@style]/input&apos;)
&gt;&gt;&gt; sel
    [&lt;Selector xpath=&apos;//div[@style]/input&apos; data=u&apos;&lt;input name=&quot;_next&quot; type=&quot;hidden&quot; value=&apos;&gt;,
     &lt;Selector xpath=&apos;//div[@style]/input&apos; data=u&apos;&lt;input name=&quot;_formkey&quot; type=&quot;hidden&quot; val&apos;&gt;,
     &lt;Selector xpath=&apos;//div[@style]/input&apos; data=u&apos;&lt;input name=&quot;_formname&quot; type=&quot;hidden&quot; va&apos;&gt;]
&gt;&gt;&gt; fd = dict(zip(sel.xpath(&apos;./@name&apos;).extract(), sel.xpath(&apos;./@value&apos;).extract()))
&gt;&gt;&gt; fd
    {u&apos;_formkey&apos;: u&apos;9c751a58-3dc2-489f-bf7b-93c31fa00c7f&apos;,
     u&apos;_formname&apos;: u&apos;login&apos;,
     u&apos;_next&apos;: u&apos;/places/default/index&apos;}
&gt;&gt;&gt; fd[&apos;email&apos;] = &quot;liushuo@webscraping.com&quot;
&gt;&gt;&gt; fd[&apos;password&apos;] = &quot;12345678&quot;
&gt;&gt;&gt; fd
    {u&apos;_formkey&apos;: u&apos;9c751a58-3dc2-489f-bf7b-93c31fa00c7f&apos;,
     u&apos;_formname&apos;: u&apos;login&apos;,
     u&apos;_next&apos;: u&apos;/places/default/index&apos;,
     &apos;email&apos;: &apos;liushuo@webscraping.com&apos;,
     &apos;password&apos;: &apos;12345678&apos;}
&gt;&gt;&gt; from scrapy.http import FormRequest
&gt;&gt;&gt; request = FormRequest(&quot;http://example.webscraping.com/places/default/user/login&quot;, formdata=fd)

&gt;&gt;&gt; fetch(request)
&gt;&gt;&gt; response.url
    &apos;http://example.webscraping.com/places/default/index&apos;
&gt;&gt;&gt; &quot;Welcome&quot; in response.text
    True
</code></pre></li>
</ol>
<ol>
<li><p>调用 FormRequest 的 from_response 方法</p>
<p> 调用时, 只需传入一个 Response 对象作为第一个参数, 该方法会解析 Response 对象所包含的页面中的 <form> 元素, 帮助用户创建 FormRequest 对象, 并将隐藏 <input> 中的信息自动填入表单数据.</form></p>
<pre><code>$ scrapy shell http://example.webscraping.com/places/default/user/login

&gt;&gt;&gt; fd = {&quot;email&quot;: &quot;liushuo@webscraping.com&quot;, &quot;password&quot;: &quot;12345678&quot;}
&gt;&gt;&gt; from scrapy.http import FormRequest
&gt;&gt;&gt; req = FormRequest.from_response(response, fd)
&gt;&gt;&gt; fetch(req)

&gt;&gt;&gt; response.url
    &apos;http://example.webscraping.com/places/default/index&apos;
</code></pre></li>
</ol>
<h3 id="1-实现登录-Spider"><a href="#1-实现登录-Spider" class="headerlink" title="1. 实现登录 Spider"></a>1. 实现登录 Spider</h3><pre><code>class LoginSpider(scrapy.Spider):
    name = &quot;login&quot;
    allowed_domains = [&quot;example.webscraping.com&quot;]
    start_urls = [&quot;http://example.webscraping.com/places/default/user/profile&quot;]

    login_url = &quot;http://example.webscraping.com/places/default/user/login&quot;

    def parse(self, response):
        keys = response.css(&quot;table label::text&quot;).re(&quot;(.+):&quot;)
        values = response.css(&quot;table td.w2p_fw::text&quot;).extract()

        yield dict(zip(keys, values))

    def start_requests(self):
        yield scrapy.Request(self.login_url, callback=self.login)

    def login(self, response):
        fd = {&quot;email&quot;: &quot;liushuo@webscraping.com&quot;, &quot;password&quot;: &quot;12345678&quot;}
        yield scrapy.http.FormRequest.from_response(response, formdata=fd, callback=self.parse_login)

    def parse_login(self, response):
        # 如果 if 判断成功, 调用基类的 start_request() 方法, 继续爬取 start_urls 中的页面.
        if &quot;Welcome&quot; in response.text:
            yield from super().start_requests()     # Python 3 语法
</code></pre><h3 id="2-识别验证码"><a href="#2-识别验证码" class="headerlink" title="2. 识别验证码"></a>2. 识别验证码</h3><ol>
<li><p>OCR 识别: <code>tesseract-ocr</code></p>
<p> pytesseract 可以识别的验证码比较简单, 对于某些复杂的验证码, pytesseract 识别率很低, 或无法识别.</p>
<p> 基本安装与使用</p>
<pre><code># 安装
$ yum install tesseract -y
$ pip install pillow
$ pip install pytesseract

# 使用
&gt;&gt;&gt; from PIL import Image
&gt;&gt;&gt; import pytesseract
&gt;&gt;&gt; img = Image.open(&quot;code.png&quot;)
&gt;&gt;&gt; img = img.convert(&quot;L&quot;)          # 为提高图像识别率, 把图片转换成黑白图.
&gt;&gt;&gt; pytesseract.image_to_string(img)
</code></pre><p> 代码示例:</p>
<pre><code>import json
from PIL import Image
from io import BytesIO
import pytesseract

class CaptchaLoginSpider(scrapy.Spider):
    name = &quot;login_captcha&quot;
    start_urls = [&quot;http://xxx.com&quot;]

    login_url = &quot;http://xxx.com/login&quot;
    user = &quot;tom@example.com&quot;
    password = &quot;123456&quot;

    def parse(self, response):
        pass

    def start_requests(self):
        yield scrapy.Request(self.login_url, callback=self.login, dont_filter=True)

    def login(self, response):
        &quot;&quot;&quot;
        该方法即是登录页面的解析方法, 又是下载验证码图片的响应处理函数.
        :param response:
        :return:
        &quot;&quot;&quot;

        # 如果 response.meta[&quot;login_response&quot;] 存在, 当前 response 为验证码图片的响应,
        # 否则, 当前 response 为登录页面的响应.

        login_response = response.meta.get(&quot;login_response&quot;)

        if not login_response:
            # 此时 response 为 登录页面的响应, 从中提取验证码图片的 url, 下载验证码图片
            captchaUrl = response.css(&quot;label.field.prepend-icon img::attr(src)&quot;).extract_first()
            captchaUrl = response.urljoin(captchaUrl)

            yield scrapy.Request(captchaUrl, callback=self.login, meta={&quot;login_response&quot;: response}, dont_filter=True)

        else:
            # 此时, response 为验证码图片的响应, response.body 为图片二进制数据,
            # login_response 为登录页面的响应, 用其构造表单请求并发送.
            formdata = {
                &quot;email&quot;: self.user,
                &quot;password&quot;: self.password,
                &quot;code&quot;: self.get_captcha_by_ocr(response.body)
            }

            yield scrapy.http.FormRequest.from_response(login_response,
                                                        callback=self.parse_login,
                                                        formdata=formdata,
                                                        dont_click=True)

    def parse_login(self, response):
        info = json.loads(response.text)

        if info[&quot;error&quot;] == &quot;0&quot;:
            scrapy.log.logger.info(&quot;登录成功!&quot;)
            return super().start_requests()
        scrapy.log.logger.info(&quot;登录失败!&quot;)
        return self.start_requests()

    def get_captha_by_ocr(self, data):
        img = Image.open(BytesIO(data))
        img = img.convert(&quot;L&quot;)
        captcha = pytesseract.image_to_string(img)
        img.close()

        return captcha
</code></pre></li>
<li><p>网络平台识别</p>
<p> 阿里云市场提供很多验证码识别平台, 他们提供了 HTTP 服务接口, 用户通过 HTTP 请求将验证码图片发送给平台, 平台识别后将结果通过 HTTP 响应返回.</p>
</li>
<li><p>人工识别</p>
<p> 在 Scrapy 下载完验证码图片后, 调用 Image.show 方法将图片显示出来, 然后调用 Python 内置的 Input 函数, 等待用户肉眼识别后输入识别结果.</p>
<pre><code>def get_captha_by_user(self, data):
    img = Image.open(BytesIO(data))
    img.show()
    captha = input(&quot;请输入验证码: &quot;)
    img.close()
    return captha
</code></pre></li>
</ol>
<h3 id="3-Cookie-登录-amp-amp-CookiesMiddleware"><a href="#3-Cookie-登录-amp-amp-CookiesMiddleware" class="headerlink" title="3. Cookie 登录 &amp;&amp; CookiesMiddleware"></a>3. Cookie 登录 &amp;&amp; CookiesMiddleware</h3><p>在使用浏览器登录网站后, 包含用户身份信息的 Cookie 会被浏览器保存到本地, 如果 Scrapy 爬虫能直接使用浏览器的 Cookie 发送 HTTP 请求, 就可以绕过提交表单登录的步骤.</p>
<h4 id="3-1-browsercookie"><a href="#3-1-browsercookie" class="headerlink" title="3.1 browsercookie"></a>3.1 browsercookie</h4><p>第三方 Python 库 <code>browsercookie</code> 便可以获取 Chrome 和 Firefox 浏览器中的 Cookie.</p>
<pre><code>$ pip install browsercookie

&gt;&gt;&gt; import browsercookie
&gt;&gt;&gt; chrome_cookiejar = browsercookie.chrome()
&gt;&gt;&gt; firefox_cookiejar = browsercookie.firefox()

&gt;&gt;&gt; type(chrome_cookiejar)
    http.cookiejar.CookieJar

&gt;&gt;&gt; for cookie in chrome_cookiejar:     # 对 http.cookiejar.CookieJar 对象进行迭代, 可以访问其中的每个 Cookie 对象.
        print cookie
</code></pre><h4 id="3-2-CookiesMiddleware"><a href="#3-2-CookiesMiddleware" class="headerlink" title="3.2 CookiesMiddleware"></a>3.2 CookiesMiddleware</h4><pre><code>import six
import logging
from collections import defaultdict

from scrapy.exceptions import NotConfigured
from scrapy.http import Response
from scrapy.http.cookies import CookieJar
from scrapy.utils.python import to_native_str

logger = logging.getLogger(__name__)

class CookieMiddleware(object):
    &quot;&quot;&quot;
    This middleware enables working with sites that need cookies.
    &quot;&quot;&quot;

    def __init__(self, debug=False):
        &quot;&quot;&quot;
        jars 中的每一项值, 都是一个 scrapy.http.cookies.CookisJar 对象,
        CookieMiddleware 可以让 Scrapy 爬虫同时使用多个不同的 CookieJar, 即多个不同的账号.
        Request(url, meta={&quot;cookiejar&quot;: &quot;account_1&quot;}}

        :param debug:
        &quot;&quot;&quot;
        self.jars = defaultdict(CookieJar)
        self.debug = debug

    @classmethod
    def from_crawler(cls, crawler):
        &quot;&quot;&quot;
        从配置文件读取 COOKIES_ENABLED, 决定是否启用该中间件.
        :param crawler:
        :return:
        &quot;&quot;&quot;
        if not crawler.settings.getbool(&quot;COOKIES_ENABLED&quot;):
            raise NotConfigured
        return cls(crawler.settings.getbool(&quot;COOKIES_DEBUG&quot;))

    def process_request(self, request, spider):
        &quot;&quot;&quot;
        处理每一个待发送到额 Request 对象, 尝试从 request.meta[&quot;cookiejar&quot;] 获取用户指定使用的 Cookiejar,
        如果用户未指定, 就是用默认的 CookieJar(self.jars[None]).

        调用 self._get_request_cookies 方法获取发送请求 request 应携带的 Cookie 信息, 填写到 HTTP 请求头.

        :param request:
        :param spider:
        :return:
        &quot;&quot;&quot;
        if request.meta.get(&quot;dont_merge_cookies&quot;, False):
            request

        cookiejarkey = request.meta.get(&quot;cookiejar&quot;)
        jar = self.jar[cookiejarkey]
        cookies = self._get_request_cookies(jar, request)
        for cookie in cookies:
            jar.set_cookie_if_ok(cookie, request)

        # set Cookie header
        request.headers.pop(&quot;Cookie&quot;, None)
        jar.add_cookie_header(request)
        self._debug_cookie(request, spider)

    def process_response(self, request, response, spider):
        &quot;&quot;&quot;
        处理每一个 response 对象, 依然通过 request.meta[&quot;cookiejar&quot;] 获取用户指定使用的 cookiejar,
        调用 extract_cookies 方法将 HTTP 响应头部中的 Cookie 信息保存到 CookieJar 对象中.

        :param request:
        :param response:
        :param spider:
        :return:
        &quot;&quot;&quot;
        if request.meta.get(&quot;dont_merge_cookies&quot;, False):
            return response

        # extract cookies from Set-Cookie and drop invalid/expired cookies.
        cookiejarkey = request.meta.get(&quot;cookiejar&quot;)
        jar = self.jars[cookiejarkey]
        jar.extract_cookies(response, request)
        self._debug_set_cookie(response, request)

        return response

    def _debug_cookie(self, request, spider):
        if self.debug:
            cl = [to_native_str(c, errors=&quot;replace&quot;) for c in request.headers.getlist(&quot;Cookie&quot;)]

            if cl:
                cookies = &quot;\n&quot;.join(&quot;Cookie: {}\n&quot;.format(c) for c in cl)
                msg = &quot;Sending cookies to:{}\n&quot;.format(request, cookies)
                logger.debug(msg, extra={&quot;spider&quot;: spider})

    def _debug_set_cookie(self, response, spider):
        if self.debug:
            cl = [to_native_str(c, errors=&quot;replace&quot;) for c in response.headers.getlist(&quot;Set-Cookie&quot;)]

            if cl:
                cookies = &quot;\n&quot;.join(&quot;Set-Cookie: {}\n&quot;.format(c) for c in cl)
                msg = &quot;Received cookies from:{}\n&quot;.format(response, cookies)
                logger.debug(msg, extra={&quot;spider&quot;: spider})

    def _format_cookie(self, cookie):
        # build cookie string
        cookie_str = &quot;%s=%s&quot; % (cookie[&quot;name&quot;], cookie[&quot;value&quot;])

        if cookie.get(&quot;path&quot;, None):
            cookie_str += &quot;;Path=%s&quot; % cookie[&quot;path&quot;]

        if cookie.get(&quot;domain&quot;, None):
            cookie_str += &quot;;Domain=%s&quot; % cookie[&quot;domain&quot;]

        return cookie_str

    def _get_request_cookies(self, jar, request):
        if isinstance(request.cookies, dict):
            cookie_list = [{&quot;name&quot;: k, &quot;value&quot;: v} for k, v in six.iteritems(request.cookies)]
        else:
            cookie_list = request.cookies

        cookies = [self._format_cookie(x) for x in cookie_list]

        headers = {&quot;Set-Cookie&quot;: cookies}
        response = Response(request.url, headers=headers)

        return jar.make_cookies(response, request)
</code></pre><h4 id="3-3-实现-BrowserCookieMiddleware"><a href="#3-3-实现-BrowserCookieMiddleware" class="headerlink" title="3.3 实现 BrowserCookieMiddleware"></a>3.3 实现 BrowserCookieMiddleware</h4><p>利用 browsercookie 对 CookieMiddleware 进行改良.</p>
<pre><code>import browsercookie
from scrapy.downloadermiddlewares.cookies import CookiesMiddleware

class BrowserCookiesMiddleware(CookiesMiddleware):
    def __init__(self, debug=False):
        super().__init__(debug)
        self.load_browser_cookies()

    def load_browser_cookies(self):
        # for chrome
        jar = self.jars[&quot;chrome&quot;]
        chrome_cookies = browsercookie.chrome()
        for cookie in chrome_cookies:
            jar.set_cookie(cookie)

        # for firefox
        jar = self.jars[&quot;firefox&quot;]
        firefox_cookies = browsercookie.firefox()
        for cookie in firefox_cookies:
            jar.set_cookie(cookie)
</code></pre><p>使用示例:</p>
<pre><code># settings.py
USER_AGENT = &quot;Mozilla/5.0 (X11; CrOS i686 3912.101.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.116 Safari/537.36&quot;
DOWNLAODER_MIDDLEWARES = {
    &quot;scrapy.downloademiddleware.cookies.CookiesMiddleware&quot;: None,
    &quot;browser_cookie.middlewares.BrowserCookiesMiddleware&quot;: 701
}

$ scrapy shell
&gt;&gt;&gt; from scrapy import Request
&gt;&gt;&gt; url = &quot;https://www.zhihu.com/settings/profile&quot;
&gt;&gt;&gt; fetch(Request(url, meta={&quot;cookiejar&quot;: &apos;chrome&apos;}))
&gt;&gt;&gt; view(response)
</code></pre><h2 id="十-爬取动态页面-Splash-渲染引擎"><a href="#十-爬取动态页面-Splash-渲染引擎" class="headerlink" title="十. 爬取动态页面: Splash 渲染引擎"></a>十. 爬取动态页面: Splash 渲染引擎</h2><p><a href="http://splash.readthedocs.io/en/stable/" target="_blank" rel="noopener">Splash</a> 是 Scrapy 官方推荐的 javascript 渲染引擎, 他是用 Webkit 开发的轻量级无界面浏览器, 提供基于 http 接口的 javascript 渲染服务, 支持如下功能:</p>
<ul>
<li>为用户返回经过渲染的 HTML 页面或页面截图</li>
<li>并发渲染多个页面</li>
<li>关闭图片加载, 加速渲染</li>
<li>在页面中执行用户自定义的 javascript 代码.</li>
<li>指定用户自定义的渲染脚本(lua), 功能类似于 PhantomJS.</li>
</ul>
<h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><pre><code># 安装 docker
$ yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine -y

$ yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
$ yum install -y yum-utils device-mapper-persistent-data lvm2
$ yum install docker-ce
$ systemctl start docker

# 获取镜像
$ docker pull scrapinghub/splash

# 运行 splash 服务
$ docker run -p 8051:8051 -p 8050:8050 scrapinghub/splash
</code></pre><h3 id="2-render-html-提供-javascript-渲染服务"><a href="#2-render-html-提供-javascript-渲染服务" class="headerlink" title="2. render.html : 提供 javascript 渲染服务"></a>2. render.html : 提供 javascript 渲染服务</h3><table>
<thead>
<tr>
<th>服务端点</th>
<th>render.html</th>
</tr>
</thead>
<tbody>
<tr>
<td>请求地址</td>
<td><a href="http://localhost:8051/render.html" target="_blank" rel="noopener">http://localhost:8051/render.html</a></td>
</tr>
<tr>
<td>请求方式</td>
<td>GET/POST</td>
</tr>
<tr>
<td>返回类型</td>
<td>html</td>
</tr>
</tbody>
</table>
<p>参数列表:</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>是否必选</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>url</td>
<td>必选</td>
<td>string</td>
<td>需要渲染的页面 url</td>
</tr>
<tr>
<td>timeout</td>
<td>可选</td>
<td>float</td>
<td>渲染页面超时时间</td>
</tr>
<tr>
<td>proxy</td>
<td>可选</td>
<td>string</td>
<td>代理服务器地址</td>
</tr>
<tr>
<td>wait</td>
<td>可选</td>
<td>float</td>
<td>等待页面渲染的时间</td>
</tr>
<tr>
<td>images</td>
<td>可选</td>
<td>integer</td>
<td>是否下载图片, 默认为 1</td>
</tr>
<tr>
<td>js_source</td>
<td>可选</td>
<td>string</td>
<td>用自定义的 javascript 代码, 在页面渲染前执行</td>
</tr>
</tbody>
</table>
<p>示例: 使用 request 库调用 render.html 渲染页面.    </p>
<pre><code>&gt;&gt;&gt; import requests
&gt;&gt;&gt; from scrapy.selector import  Selector
&gt;&gt;&gt; splash_url = &quot;http://localhost:8050/render.html&quot;
&gt;&gt;&gt; args = {&quot;url&quot;: &quot;http://quotes.toscrape.com/js&quot;, &quot;timeout&quot;:5,&quot;image&quot;:0}
&gt;&gt;&gt; response = requests.get(splash_url, params=args)
&gt;&gt;&gt; sel = Selector(response)
&gt;&gt;&gt; sel.css(&quot;div.quote span.text::text&quot;).extract()
</code></pre><h3 id="3-execute-指定用户自定义的-lua-脚本-利用该断点可在页面中执行-javascript-代码"><a href="#3-execute-指定用户自定义的-lua-脚本-利用该断点可在页面中执行-javascript-代码" class="headerlink" title="3. execute : 指定用户自定义的 lua 脚本, 利用该断点可在页面中执行 javascript 代码."></a>3. execute : 指定用户自定义的 lua 脚本, 利用该断点可在页面中执行 javascript 代码.</h3><p>在爬去某些页面时, 希望在页面中执行一些用户自定义的 javascript 代码, 例如, 用 javascript 模拟点击页面中的按钮, 或调用页面中的 javascript 函数与服务器交互, 利用 Splash 的 execute 端点可以实现这样的功能.</p>
<table>
<thead>
<tr>
<th>服务端点</th>
<th>execute</th>
</tr>
</thead>
<tbody>
<tr>
<td>请求地址</td>
<td><a href="http://localhost:8051/execute" target="_blank" rel="noopener">http://localhost:8051/execute</a></td>
</tr>
<tr>
<td>请求方式</td>
<td>POST</td>
</tr>
<tr>
<td>返回类型</td>
<td>自定义</td>
</tr>
</tbody>
</table>
<p>参数 :</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>是否必选</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>lua_source</td>
<td>必选</td>
<td>string</td>
<td>用户自定义的 Lua 脚本</td>
</tr>
<tr>
<td>timeout</td>
<td>可选</td>
<td>float</td>
<td>渲染页面超时时间</td>
</tr>
<tr>
<td>proxy</td>
<td>可选</td>
<td>string</td>
<td>代理服务器地址</td>
</tr>
</tbody>
</table>
<p>可以将  execute 端点的服务看做一个可用 lua 语言编程的浏览器, 功能类似于 PhantomJS. 使用时需传递一个用户自定义的 Lua 脚本给 Spalsh, 该 Lua 脚本中包含用户想要模拟的浏览器行为. 如</p>
<ul>
<li>打开某 url 地址的页面</li>
<li>等待页面加载完成</li>
<li>执行 javascript 代码</li>
<li>获取 HTTP 响应头部</li>
<li>获取 Cookie</li>
</ul>
<p>用户定义的 lua 脚本必须包含一个 <code>main</code> 函数作为程序入口, main 函数被调用时传入一个 splash 对象(lua中的对象), 用户可以调用该对象上的方法操作 Splash. main 函数的返回值可以使 字符串, 也可以是 lua 中的表(类似 python 字典), 表会被编码成 json 串.</p>
<p><code>Splash</code> 对象常用属性和方法</p>
<ul>
<li><p><code>splash.args</code>属性</p>
<p>  用户传入参数的表, 通过该属性可以访问用户传入的参数, 如 <code>splash.args.url</code></p>
</li>
<li><p><code>splash.js_enabled</code></p>
<p>  用于开启/禁止 javascript 渲染, 默认为 True</p>
</li>
<li><p><code>splash.images_enabled</code></p>
<p>  用于开启/禁止 图片加载, 默认为 True</p>
</li>
<li><p><code>splash:go()</code></p>
<p>  <code>splash:go(url, baseurl=nil, headers=nil, http_method=&quot;GET&quot;, body=nil, formdata=nil)</code> 类似于在浏览器中打开某 url 地址的页面, 页面所需资源会被加载, 并进行 javascript 渲染, 可以通过参数指定 HTTP 请求头部, 请求方法, 表单数据等.</p>
</li>
<li><p><code>splash:wait()</code></p>
<p>  <code>splash:wait(time, cancel_on_redirect=false, cancel_on_error=true)</code> 等待页面渲染, time 参数为等待的秒数.</p>
</li>
<li><p><code>splash:evaljs()</code></p>
<p>  <code>splash:evaljs(snippet)</code> 在当前页面下, 执行一段 javascript 代码, 并返回<strong>最后一句表达式的值</strong>.</p>
</li>
<li><p><code>splash:runjs()</code></p>
<p>  <code>splash:runjs(snippet)</code> 在当前页面下, 执行一段 javascript 代码, 与 evaljs 相比, 该函数只执行代码, <strong>不返回值</strong>.</p>
</li>
<li><p><code>splash:url()</code></p>
<p>  获取当前页面的 url</p>
</li>
<li><p><code>splash:html()</code></p>
<p>  获取当前页面的 html 文本.</p>
</li>
<li><p><code>splash:get_cookits()</code></p>
<p>  获取全部 cookie 信息.</p>
</li>
</ul>
<p>示例代码: requests 库调用 execute 端点服务</p>
<pre><code>&gt;&gt;&gt; import json
&gt;&gt;&gt; lua_script = &quot;&quot;&quot;
    ...: function main(splash)
    ...:     splash:go(&apos;http://example.com&apos;)
    ...:     splash:wait(0.5)
    ...:     local title = splash:evaljs(&quot;document.title&quot;)
    ...:     return {title=title}
    ...: end &quot;&quot;&quot;
&gt;&gt;&gt; splash_url = &quot;http://localhost:8050/execute&quot;
&gt;&gt;&gt; headers = {&quot;content-type&quot;: &quot;application/json&quot;}
&gt;&gt;&gt; data = json.dumps({&quot;lua_source&quot;: lua_script})
&gt;&gt;&gt; response = requests.post(splash_url, headers=headers, data=data)

&gt;&gt;&gt; response.content
&gt;&gt;&gt; &apos;{&quot;title&quot;: &quot;Example Domain&quot;}&apos;

&gt;&gt;&gt; response.json()
&gt;&gt;&gt; {u&apos;title&apos;: u&apos;Example Domain&apos;}
</code></pre><h3 id="4-scrapy-splash"><a href="#4-scrapy-splash" class="headerlink" title="4. scrapy-splash"></a>4. scrapy-splash</h3><ol>
<li><p>安装</p>
<pre><code>$ pip install scrapy-splash
</code></pre></li>
<li><p>配置</p>
<pre><code>$ cat settings.py
    # Splash 服务器地址
    SPLASH_URL = &quot;http://localhost:8050&quot;

    # 开启 Splash 的两个下载中间件并调整 HttpCompressionMiddleware 的次序
    DOWNLOADER_MIDDLEWARES = {
        &quot;scrapy_splash.SplashCookiesMiddleware&quot;: 723,
        &quot;scrapy_splash.SplashMiddleware&quot;: 725,
        &quot;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&quot;: 810,
    }

    # 设置去重过滤器
    DUPEFILTER_CLASS = &quot;scrapy_splash.SplashAwareDupeFilter&quot;

    # 用来支持 cache_args (可选)
    SPIDER_MIDDLEWARES = {
        &quot;scrapy_splash.SplashDeduplicateArgsMiddleware&quot;: 100,
    }
</code></pre></li>
<li><p>使用 </p>
<p> Scrapy_splash 调用 Splash 服务非常简单. scrapy_Splash 中定义了一个 <code>SplashRequest</code> 类, 用户只需使用 <code>scrapy_splash.SplashRequest</code> (替代 scrapy.Request) 提交请求即可.</p>
<p> <code>SplashRequest</code> 构造器方法参数</p>
<ul>
<li><code>url</code> : 待爬去页面的 url</li>
<li><code>headers</code> : 请求 headers, 同 scrapy.Request.</li>
<li><code>cookies</code> : 请求 cookie, 同 scrapy.Request.</li>
<li><code>args</code> : 传递给 Splash 的参数(除 url), 如 wait, timeout, images, js_source 等</li>
<li><code>cache_args</code> : 如果 args 中的某些参数每次调用都重复传递, 并且数据量巨大, 此时可以把该参数名填入 cache_args 列表中, 让 Splash 服务器缓存该参数. 如 <code>SplashRequest(url, args={&quot;js_source&quot;: js, &quot;wait&quot;: 0.5}, cache_args=[&quot;js_source&quot;])</code></li>
<li><code>endpoint</code> : Splash 服务端点, 默认为 <code>render.html</code>, 即 javascript 渲染服务. 该参数可以设置为 <code>render.json</code>, <code>render.har</code>, <code>render.png</code>, <code>render.jpeg</code>, <code>execute</code> 等. 详细参考文档.</li>
<li><code>splash_url</code> : Splash 服务器地址, 默认为 None, 即使用配置文件中的 <code>SPLASH_URL</code> 地址.</li>
</ul>
</li>
</ol>
<h3 id="5-代码示例"><a href="#5-代码示例" class="headerlink" title="5. 代码示例"></a>5. 代码示例</h3><ol>
<li><p>quote 名人名言爬取</p>
<pre><code>import scrapy
from scrapy_splash import SplashRequest
</code></pre></li>
</ol>
<pre><code>class QuotesSpider(scrapy.Spider):
    name = &apos;quotes&apos;
    allowed_domains = [&apos;quotes.toscrape.com&apos;]
    start_urls = [&apos;http://quotes.toscrape.com/js&apos;]

    splash_base_args = {&quot;images&quot;: 0, &quot;timeout&quot;: 3}

    def start_requests(self):
        for url in self.start_urls:
            yield SplashRequest(url, args=self.splash_base_args)

    def parse(self, response):
        for sel in response.css(&quot;div.quote&quot;):
            quote = sel.css(&quot;span.text::text&quot;).extract_first()
            author = sel.css(&quot;small.author::text&quot;).extract_first()
            yield {&quot;quote&quot;: quote, &quot;author&quot;: author}

        href = response.css(&quot;li.next &gt; a::attr(href)&quot;).extract_first()
        if href:
            url = response.urljoin(href)
            yield SplashRequest(url, args=self.splash_base_args)
</code></pre><ol>
<li><p>jd 图书 爬取</p>
<pre><code>import scrapy
from scrapy import Request
from scrapy_splash import SplashRequest

lua_script = &quot;&quot;&quot;
function main(splash)
    splash:go(splash.args.url)
    splash:wait(2)
    splash:runjs(&quot;document.getElementsByClassName(&apos;page&apos;)[0].scrollIntoView(true)&quot;)
    splash:wait(2)
    return splash:html()
end
&quot;&quot;&quot;

class JdBookSpider(scrapy.Spider):
    name = &apos;jd_book&apos;
    allowed_domains = [&apos;search.jd.com&apos;]

    base_url = &quot;https://search.jd.com/Search?keyword=python&amp;enc=utf-8&amp;book=y&amp;wq=python&quot;

    def start_requests(self):
        # 请求第一个页面, 无需渲染 js
        yield Request(self.base_url, callback=self.parse_url, dont_filter=True)

    def parse_url(self, response):
        # 获取商品总数, 计算出总页数.
        total = int(response.css(&quot;span#J_resCount::text&quot;).re_first(&quot;(\d+)\D?&quot;))
        pageNum = total // 60 + (1 if total % 60 else 0)

        # 构造每一页的 url, 向 Splash 端点发送请求
        for i in xrange(pageNum):
            url = &quot;%s&amp;page=%s&quot; % (self.base_url, 2*i + 1)
            headers = {&quot;refer&quot;: self.base_url}
            yield SplashRequest(url, endpoint=&quot;execute&quot;, headers=headers,
                                args={&quot;lua_source&quot;: lua_script}, cache_args=[&quot;lua_source&quot;])

    def parse(self, response):
        # 获取单个页面中每本书的名字和价格
        for sel in response.css(&quot;ul.gl-warp.clearfix &gt; li.gl-item&quot;):
            yield {
                &quot;name&quot;: sel.css(&quot;div.p-name&quot;).xpath(&quot;string(.//em)&quot;).extract_first(),
                &quot;price&quot;: sel.css(&quot;div.p-price i::text&quot;).extract_first()
            }

$ vim settings.py

    USER_AGENT = u&apos;Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36&apos;
</code></pre></li>
</ol>
<h2 id="十一-HTTP-代理"><a href="#十一-HTTP-代理" class="headerlink" title="十一. HTTP 代理"></a>十一. HTTP 代理</h2><p>Scrapy 内部提供了一个下载中间件<code>HttpProxyMiddleware</code>, 专门用于给 Scrapy 设置代理, 他默认是启动的, 他会在系统环境变量中搜索当前系统代理(名称格式为<strong>xxx_proxy</strong>的环境变量), 作为 Scrapy 爬虫使用的带来.</p>
<pre><code>$ export http_proxy=&quot;http://192.168.1.1:8000&quot;
$ export https_proxy=&quot;http://192.168.1.1:8001&quot;

# 包含用户名和密码
$ export https_proxy=&quot;http://username:password@192.168.1.1:8001&quot;

$ curl http(s)://httpbin.org/ip     # 返回一个包含请求源 ip 地址信息的额 json 字符串.
</code></pre><p>Scrapy 中为一个请求设置代理的本质就是将代理服务器的 url 填写到 <code>request.meta[&quot;proxy&quot;]</code>.</p>
<pre><code>class HttpProxyMiddleware(object):
    ...
    def _set_proxy(self, request, scheme):
        creds, proxy = self.proxies[scheme]
        request.meta[&quot;proxy&quot;] = proxy
        if creds:
            # 如果需要认证, 传递包含用户账号和密码的身份验证信息
            request.headers[&quot;Proxy-Authorization&quot;] = b&quot;Basic&quot; + creds 

# 手动实现
$ scrapy shell

&gt;&gt;&gt; from scrapy import Request
&gt;&gt;&gt; import base64

&gt;&gt;&gt; req = Request(&quot;http://httpbin.org/ip&quot;, meta={&quot;proxy&quot;: &quot;http://192.168.1.1:8000&quot;})
&gt;&gt;&gt; user = &quot;tom&quot;
&gt;&gt;&gt; password = &quot;tom123&quot;
&gt;&gt;&gt; user_passwd = (&quot;%s:%s&quot; % (user, password)).encode(&quot;utf8&quot;)
&gt;&gt;&gt; req.headers[&quot;Proxy-Authorization&quot;] = b&quot;Basic&quot; + base64.b64encode(user_passwd)
&gt;&gt;&gt; fetch(req)
</code></pre><h3 id="1-抓取免费代理"><a href="#1-抓取免费代理" class="headerlink" title="1. 抓取免费代理:"></a>1. 抓取免费代理:</h3><p>代理网站:</p>
<ul>
<li><code>http://proxy-list.org</code></li>
<li><code>https://free-proxy-list.net</code></li>
<li><code>http://www.xicidaili.com</code></li>
<li><code>http://www.proxy360.cn</code></li>
<li><code>http://www.kuaidaili.com</code></li>
</ul>
<p>获取西祠代理代码</p>
<pre><code># settings.py
USER_AGENT = &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)&quot;

# spider.py
import json

import scrapy
from scrapy import Request


class XiciSpider(scrapy.Spider):
    name = &apos;xici&apos;
    allowed_domains = [&apos;www.xicidaili.com&apos;]
    # start_urls = [&apos;http://www.xicidaili.com/nn/&apos;]
    base_url = &quot;http://www.xicidaili.com/nn/%s&quot;
    check_url = &quot;%s://httpbin.org/ip&quot;

    def start_requests(self):
        for i in xrange(1, 5):
            yield Request(self.base_url % i)

    def parse(self, response):
        for sel in response.xpath(&quot;//table[@id=&apos;ip_list&apos;]/tr[position()&gt;1]&quot;):
            ip = sel.css(&apos;td:nth-child(2)::text&apos;).extract_first()
            port = sel.css(&apos;td:nth-child(3)::text&apos;).extract_first()
            scheme = sel.css(&apos;td:nth-child(6)::text&apos;).extract_first().lower()

            url = self.check_url % scheme
            proxy = &quot;%s://%s:%s&quot; % (scheme, ip, port)

            meta = {
                &quot;proxy&quot;: proxy,
                &quot;dont_retry&quot;: True,
                &quot;download_timeout&quot;: 10,

                &quot;_proxy_scheme&quot;: scheme,
                &quot;_proxy_ip&quot;: ip
            }

            yield Request(url, callback=self.check_available, meta=meta, dont_filter=True)

    def check_available(self, response):
        proxy_ip = response.meta[&quot;_proxy_ip&quot;]

        if proxy_ip == json.loads(response.text)[&quot;origin&quot;]:
            yield {
                &quot;proxy_scheme&quot;: response.meta[&quot;_proxy_scheme&quot;],
                &quot;proxy&quot;: response.meta[&quot;proxy&quot;]
            }
</code></pre><h3 id="2-基于-HttpProxyMiddleware-实现随机代理"><a href="#2-基于-HttpProxyMiddleware-实现随机代理" class="headerlink" title="2. 基于 HttpProxyMiddleware 实现随机代理"></a>2. 基于 HttpProxyMiddleware 实现随机代理</h3><pre><code># middlewares.py
class RandomHttpProxyMiddleware(HttpProxyMiddleware):
    def __init__(self, auth_encoding=&quot;latin-1&quot;, proxy_list_file=None):
        if not proxy_list_file:
            raise NotConfigured

        self.auth_encoding = auth_encoding

        # 用两个列表维护 HTTP 和 HTTPS 代理, {&quot;http&quot;: [...], &quot;https&quot;: [...]}
        self.proxies = defaultdict(list)

        with open(proxy_list_file) as f:
            proxy_list = json.load(f)
            for proxy in proxy_list:
                scheme = proxy[&quot;proxy_scheme&quot;]
                url = proxy[&quot;proxy&quot;]
                self.proxies[scheme].append(self._get_proxy(url, scheme))

    @classmethod
    def from_crawler(cls, crawler):
        auth_encoding = crawler.settings.get(&quot;HTTPPROXY_AUTH_ENCODING&quot;, &quot;latin-1&quot;)
        proxy_list_file = crawler.settings.get(&quot;HTTPPROXY_PROXY_LIST_FILE&quot;)

        return cls(auth_encoding, proxy_list_file)

    def _set_proxy(self, request, scheme):
        creds, proxy = random.choice(self.proxies[scheme])
        request.meta[&quot;proxy&quot;] = proxy
        if creds:
            request.headers[&quot;Proxy-Authorization&quot;] = b&quot;Basic&quot; + creds

# spider.py : 测试随机 proxy 是否 work
import json
import scrapy
from scrapy import Request

class TestRandomProxySpider(scrapy.Spider):
    name = &quot;random_proxy&quot;

    def start_requests(self):
        for _ in range(100):
            yield Request(&quot;http://httpbin.org/ip&quot;, dont_filter=True)
            yield Request(&quot;https://httpbin.org/ip&quot;, dont_filter=True)

    def parse(self, response):
        print json.loads(response.text)

# settings.py
DOWNLOADER_MIDDLEWARES = {
   &apos;proxy_example.middlewares.RandomHttpProxyMiddleware&apos;: 543,
}
HTTPPROXY_PROXY_LIST_FILE = &quot;proxy.json&quot;
</code></pre><h3 id="3-实战-豆瓣电影"><a href="#3-实战-豆瓣电影" class="headerlink" title="3. 实战: 豆瓣电影"></a>3. 实战: 豆瓣电影</h3><pre><code># Spider.py
import json
import re

import scrapy
from scrapy import Request


class DmovieSpider(scrapy.Spider):

    BASE_URL = &quot;https://movie.douban.com/j/search_subjects?type=movie&amp;tag=%s&amp;sort=recommend&amp;page_limit=%s&amp;page_start=%s&quot;

    MOVIE_TAG = &quot;豆瓣高分&quot;
    PAGE_LIMIT = 20
    page_start = 0

    name = &apos;dmovie&apos;
    allowed_domains = [&apos;movie.douban.com&apos;]
    start_urls = [BASE_URL % (MOVIE_TAG, PAGE_LIMIT, page_start)]

    def parse(self, response):
        infos = json.loads(response.body.decode(&quot;utf-8&quot;))

        for movie_info in infos[&quot;subjects&quot;]:
            movie_item = {}

            movie_item[&quot;片名&quot;] = movie_info[&quot;title&quot;]
            movie_item[&quot;评分&quot;] = movie_info[&quot;rate&quot;]

            yield Request(movie_info[&quot;url&quot;], callback=self.parse_movie, meta={&quot;_movie_item&quot;: movie_item})

        if len(infos[&quot;subjects&quot;]) == self.PAGE_LIMIT:
            self.page_start += self.PAGE_LIMIT
            url = self.BASE_URL % (self.MOVIE_TAG, self.PAGE_LIMIT, self.page_start)
            yield Request(url)

    def parse_movie(self, response):
        movie_item = response.meta[&quot;_movie_item&quot;]
        info = response.css(&quot;div.subject div#info&quot;).xpath(&quot;string(.)&quot;).extract_first()

        fields = [s.strip().replace(&quot;:&quot;, &quot;&quot;) for s in response.css(&quot;div#info span.pl::text&quot;).extract()]
        values = [re.sub(&quot;\s+&quot;, &quot;&quot;, s.strip()) for s in re.split(&apos;\s*(?:%s):\s*&apos; % &quot;|&quot;.join(fields), info)][1:]

        movie_item.update(dict(zip(fields, values)))

        yield movie_item

# settings.py
DOWNLOADER_MIDDLEWARES = {
   &apos;douban_movie.middlewares.RandomHttpProxyMiddleware&apos;: 543,
}

HTTPPROXY_PROXY_LIST_FILE = &quot;proxy.json&quot;
USER_AGENT = &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)&quot;

DOWNLOAD_DELAY = 2
ROBOTSTXT_OBEY = False

# middleware.py
class RandomHttpProxyMiddleware(HttpProxyMiddleware):
    def __init__(self, auth_encoding=&quot;latin-1&quot;, proxy_list_file=None):
        if not proxy_list_file:
            raise NotConfigured

        self.auth_encoding = auth_encoding

        # 用两个列表维护 HTTP 和 HTTPS 代理, {&quot;http&quot;: [...], &quot;https&quot;: [...]}
        self.proxies = defaultdict(list)

        with open(proxy_list_file) as f:
            proxy_list = json.load(f)
            for proxy in proxy_list:
                scheme = proxy[&quot;proxy_scheme&quot;]
                url = proxy[&quot;proxy&quot;]
                self.proxies[scheme].append(self._get_proxy(url, scheme))

    @classmethod
    def from_crawler(cls, crawler):
        auth_encoding = crawler.settings.get(&quot;HTTPPROXY_AUTH_ENCODING&quot;, &quot;latin-1&quot;)
        proxy_list_file = crawler.settings.get(&quot;HTTPPROXY_PROXY_LIST_FILE&quot;)

        return cls(auth_encoding, proxy_list_file)

    def _set_proxy(self, request, scheme):
        creds, proxy = random.choice(self.proxies[scheme])
        request.meta[&quot;proxy&quot;] = proxy
        if creds:
            request.headers[&quot;Proxy-Authorization&quot;] = b&quot;Basic&quot; + creds

# proxy.json
[
{&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://111.155.116.237:8123&quot;},
{&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://222.188.190.99:6666&quot;},
{&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://60.23.36.250:80&quot;},
{&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://120.79.216.57:6666&quot;},
{&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://120.92.88.202:10000&quot;},
{&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://120.79.151.197:6666&quot;},
{&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://118.114.77.47:8080&quot;},
{&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://112.74.62.69:8081&quot;},
{&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://218.93.166.4:6666&quot;},
{&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://58.216.202.149:8118&quot;},
{&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://14.118.253.233:6666&quot;}
]
</code></pre><h2 id="十二-scrapy-redis-分布式爬虫"><a href="#十二-scrapy-redis-分布式爬虫" class="headerlink" title="十二. scrapy-redis 分布式爬虫"></a>十二. scrapy-redis 分布式爬虫</h2><p>Scrapy-redis 利用 Redis 数据库重新实现了 Scrapy 中的某些组件. </p>
<ul>
<li>基于 Redis 的请求队列(优先队列, FIFO, LIFO)</li>
<li>基于 Redis 的请求去重过滤器(过滤掉重复的请求)</li>
<li>基于以上两个组件的调度器</li>
</ul>
<p>Scrapy-redis 为多个爬虫分配爬取任务的方式是: 让所有爬虫共享一个存在于 Redis 数据库中的请求队列(替代各爬虫独立的请求队列), 每个爬虫从请求队列中获取请求, 下载并解析出新请求再添加到 请求队列中, 因此, 每个爬虫即是下载任务的生产者, 又是消费者.</p>
<ol>
<li><p>搭建分布式环境</p>
<pre><code># 在所有机器上安装包
$ pip install scrapy
$ pip install scrapy-redis

# 启动redis server, 确保分布式环境中每台机器均可访问 redis-server
$ redis-cli -h REDIS_SERVER ping
</code></pre></li>
<li><p>配置项目</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># settings.py</span><br><span class="line">## 指定爬虫使用的 redis 数据库</span><br><span class="line">REDIS_URL = &quot;redis://192.168.1.10:6379&quot;</span><br><span class="line"></span><br><span class="line">## 使用 scrapy-redis 的调度器替代 scrapy 原版调度器</span><br><span class="line">SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line"></span><br><span class="line">## 使用 scrapy-redis 的 RFPDupeFilter 作为去重过滤器</span><br><span class="line">DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line"></span><br><span class="line">## 启动 scrapy_redis 的 RedisPipeline 将爬取到的数据汇总到 数据库.</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    &quot;scrapy_redis.pipelines.RedisPipeline&quot;: 300,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">## 爬虫停止后, 保留/清理 redis 中的请求队列及去重即可. True: 保留, False: 清理(默认).</span><br><span class="line">SCHEDULER_PERSIST = True</span><br></pre></td></tr></table></figure>
<p> Scrapy-redis 提供了一个新的 Spider 基类 <code>RedisSpider</code>, RedisSpider 重写了 <code>start_requests</code> 方法, 他重试从  redis 数据库的某个特定列表中获取起始爬取点, 并构造 Request 对象(dont_filter=False), 该列表的键可通过配置文件设置(<code>REDIS_START_URLS_KEY</code>), 默认为 <code>&lt;spider_name&gt;:start_urls</code>. </p>
<p> 在分布式爬取时, 用户运行所有爬虫后, 需要手动使用 Redis 命令向该列表添加起始爬取点, 从而避免重复.</p>
<pre><code># spider.py

from scrapy_redis.spiders import RedisSpider

class BooksSpider(RedisSpider):     # 爬虫 继承  RedisSpider 类
    pass

    # 注释 start_urls
    # start_urls = [&quot;http://book.toscrape.com&quot;]
</code></pre></li>
</ol>
<pre><code># 命令行 写入队列开始值.
$ redis-cli -h 192.168.1.10
&gt; lpush books:start_urls &quot;http://books.toscrape.com/&quot;
</code></pre><h2 id="十三-奇技淫巧"><a href="#十三-奇技淫巧" class="headerlink" title="十三. 奇技淫巧"></a>十三. 奇技淫巧</h2><h3 id="1-scrapy-项目的一般步骤"><a href="#1-scrapy-项目的一般步骤" class="headerlink" title="1. scrapy 项目的一般步骤"></a>1. scrapy 项目的一般步骤</h3><ol>
<li><p>创建 项目</p>
<pre><code>$ scrapy startproject PROJECT_NAME
</code></pre></li>
<li><p>创建 spider</p>
<pre><code>$ cd PROJECT_NAME
$ scrapy genspider SPIDER_NAME DOMAIN
</code></pre></li>
<li><p>封装 Item 类</p>
</li>
<li><p>完成 Spider 类</p>
</li>
<li><p>配置 settings.py</p>
<pre><code>## 指定输出序列
FEED_EXPORT_FIELDS = []
## 绕过 roobot.txt

## USER_AGENT 配置
</code></pre></li>
<li><p>编写 Pipeline, 实现 item 字段转换 : settings.py</p>
<pre><code>ITEM_PIPELINES = {
    PIPELINE_NAME: rate,
}
</code></pre></li>
<li><p>运行 crawl</p>
<pre><code>$ scrapy list    
$ scrapy crawl MySpider
</code></pre></li>
</ol>
<h3 id="2-User-Agent"><a href="#2-User-Agent" class="headerlink" title="2. User-Agent"></a>2. User-Agent</h3><ol>
<li><p><a href="http://www.cnblogs.com/yanjingnan/p/8525938.html" target="_blank" rel="noopener">使用 fake-useragent </a></p>
<p> <a href="https://github.com/hellysmile/fake-useragent" target="_blank" rel="noopener">GitHub - hellysmile/fake-useragent: up to date simple useragent faker with real world database</a></p>
<pre><code>$ pip install fake-useragent
</code></pre></li>
<li><p>各大搜索引擎的 UA</p>
<p> 可以伪装成各大搜索引擎网站的UA， 比如 <a href="https://support.google.com/webmasters/answer/1061943?hl=zh-Hans" target="_blank" rel="noopener">Google UA</a>  </p>
<p> 添加<code>referfer</code>字段为 搜索引擎网站 也是有用的，因为网站是希望被索引的，所以会放宽搜索引擎的爬取策略。</p>
</li>
<li><p><a href="http://useragentstring.com" target="_blank" rel="noopener">useragentstring.com</a></p>
</li>
</ol>
<h3 id="3-代理"><a href="#3-代理" class="headerlink" title="3. 代理"></a>3. 代理</h3><p>网上的开源代理:</p>
<pre><code>https://github.com/xiaosimao/IP_POOL
</code></pre><p>代理网站:</p>
<pre><code>http://www.kuaidaili.com/free/
http://www.66ip.cn/
http://www.goubanjia.com/free/gngn/index.shtml
http://www.xicidaili.com/

data5u
proxydb
</code></pre><p>测试网站:</p>
<pre><code>百度
https://httpbin.org/get
</code></pre><h2 id="十四-参考链接"><a href="#十四-参考链接" class="headerlink" title="十四. 参考链接"></a>十四. 参考链接</h2><ol>
<li><a href="https://www.amazon.cn/dp/B076F6W84Q/ref=sr_1_1?ie=UTF8&amp;qid=1520433164&amp;sr=8-1&amp;keywords=scrapy" target="_blank" rel="noopener">精通 Scrapy 网络爬虫</a></li>
<li><a href="https://doc.scrapy.org/en/latest/" target="_blank" rel="noopener">Scrapy 文档</a></li>
</ol>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># 1. [First Step](http://www.cnblogs.com/yanjingnan/p/7747636.html)</span><br><span class="line"></span><br><span class="line"># 2. 基本概念</span><br><span class="line">## 2.1 [命令行工具](http://www.cnblogs.com/yanjingnan/p/7748565.html)</span><br><span class="line"></span><br><span class="line">## 2.2 [spiders 爬虫](http://www.cnblogs.com/yanjingnan/p/7780968.html)</span><br><span class="line"></span><br><span class="line">## 2.3 Selectors 选择器</span><br><span class="line"></span><br><span class="line">## 2.4 Items</span><br><span class="line"></span><br><span class="line">## 2.5 Item Loaders</span><br><span class="line"></span><br><span class="line">## 2.6 Item Pipeline</span><br><span class="line"></span><br><span class="line">## 2.7 Feed exports : 输出和存储数据</span><br><span class="line"></span><br><span class="line">## 2.8 请求和响应</span><br><span class="line"></span><br><span class="line">## 2.9 连接提取</span><br><span class="line"></span><br><span class="line">## 2.10 设置, 配置</span><br><span class="line"></span><br><span class="line">## 2.11 Exceptions</span><br><span class="line"></span><br><span class="line"># 3. 内置服务</span><br><span class="line">## 3.1 Logging</span><br><span class="line"></span><br><span class="line">## 3.2 Stats Collection</span><br><span class="line"></span><br><span class="line">## 3.3 Sending e-mail</span><br><span class="line"></span><br><span class="line">## 3.4 Telnet Console</span><br><span class="line"></span><br><span class="line">## 3.5 Web Service</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 4. Q&amp;A</span><br><span class="line"></span><br><span class="line"># 5.扩展 scrapy</span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/PyPi/" rel="tag"># PyPi</a>
          
            <a href="/tags/爬虫/" rel="tag"># 爬虫</a>
          
            <a href="/tags/scrapy/" rel="tag"># scrapy</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/15/Ansible-基本原理与安装配置/" rel="next" title="Ansible-基本原理与安装配置">
                <i class="fa fa-chevron-left"></i> Ansible-基本原理与安装配置
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/03/15/crawler-Scrapy-命令行工具/" rel="prev" title="Scrapy-命令行工具">
                Scrapy-命令行工具 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Pyfdtic</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">112</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">91</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/pyfdtic" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一-Scrapy-架构组件"><span class="nav-text">一. Scrapy 架构组件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二-Spider"><span class="nav-text">二. Spider</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三-Selector-提取数据"><span class="nav-text">三. Selector 提取数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四-Item-封装数据"><span class="nav-text">四. Item 封装数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-自定义数据类-只需继承-Item-并创建一系列-Field-对象的类属性即可-类似于-ORM-创建的-Model"><span class="nav-text">1. 自定义数据类, 只需继承 Item, 并创建一系列 Field 对象的类属性即可, 类似于 ORM 创建的 Model.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Field-元数据"><span class="nav-text">2. Field 元数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码示例"><span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五-Item-Pipeline-处理数据"><span class="nav-text">五. Item Pipeline 处理数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-编写-pipeline"><span class="nav-text">1. 编写 pipeline</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-启用-Item-Pipeline"><span class="nav-text">2. 启用 Item Pipeline.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-代码示例"><span class="nav-text">3. 代码示例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Item-Pipeline-过滤重复数据"><span class="nav-text">3.1 Item Pipeline : 过滤重复数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Item-Pipeline-将数据存储-MongoDB"><span class="nav-text">3.2 Item Pipeline : 将数据存储 MongoDB</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-Item-Pipeline-将数据存储-Mysql"><span class="nav-text">3.3 Item Pipeline : 将数据存储 Mysql</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-Item-Pipeline-将数据存储-Redis"><span class="nav-text">3.4 Item Pipeline : 将数据存储 Redis</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#六-LinkExtractor-提取链接"><span class="nav-text">六. LinkExtractor 提取链接</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-使用示例"><span class="nav-text">1. 使用示例:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-链接提取规则"><span class="nav-text">2. 链接提取规则:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#七-Exporter-导出数据"><span class="nav-text">七. Exporter 导出数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-导出数据方式"><span class="nav-text">1. 导出数据方式:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-自定义数据导出格式"><span class="nav-text">2. 自定义数据导出格式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#八-下载文件-FilesPipeline-和图片-ImagesPipeline"><span class="nav-text">八. 下载文件(FilesPipeline)和图片(ImagesPipeline)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-FilesPipeline-使用方法"><span class="nav-text">1. FilesPipeline 使用方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#示例代码"><span class="nav-text">示例代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-ImagesPipeline"><span class="nav-text">2. ImagesPipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-生成缩略图"><span class="nav-text">2.1 生成缩略图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-过滤尺寸过小的图片"><span class="nav-text">2.2 过滤尺寸过小的图片</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#示例代码-1"><span class="nav-text">示例代码</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#九-模拟登陆"><span class="nav-text">九. 模拟登陆</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-实现登录-Spider"><span class="nav-text">1. 实现登录 Spider</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-识别验证码"><span class="nav-text">2. 识别验证码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Cookie-登录-amp-amp-CookiesMiddleware"><span class="nav-text">3. Cookie 登录 &amp;&amp; CookiesMiddleware</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-browsercookie"><span class="nav-text">3.1 browsercookie</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-CookiesMiddleware"><span class="nav-text">3.2 CookiesMiddleware</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-实现-BrowserCookieMiddleware"><span class="nav-text">3.3 实现 BrowserCookieMiddleware</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十-爬取动态页面-Splash-渲染引擎"><span class="nav-text">十. 爬取动态页面: Splash 渲染引擎</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-安装"><span class="nav-text">1. 安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-render-html-提供-javascript-渲染服务"><span class="nav-text">2. render.html : 提供 javascript 渲染服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-execute-指定用户自定义的-lua-脚本-利用该断点可在页面中执行-javascript-代码"><span class="nav-text">3. execute : 指定用户自定义的 lua 脚本, 利用该断点可在页面中执行 javascript 代码.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-scrapy-splash"><span class="nav-text">4. scrapy-splash</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-代码示例"><span class="nav-text">5. 代码示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十一-HTTP-代理"><span class="nav-text">十一. HTTP 代理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-抓取免费代理"><span class="nav-text">1. 抓取免费代理:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-基于-HttpProxyMiddleware-实现随机代理"><span class="nav-text">2. 基于 HttpProxyMiddleware 实现随机代理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-实战-豆瓣电影"><span class="nav-text">3. 实战: 豆瓣电影</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十二-scrapy-redis-分布式爬虫"><span class="nav-text">十二. scrapy-redis 分布式爬虫</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十三-奇技淫巧"><span class="nav-text">十三. 奇技淫巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-scrapy-项目的一般步骤"><span class="nav-text">1. scrapy 项目的一般步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-User-Agent"><span class="nav-text">2. User-Agent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-代理"><span class="nav-text">3. 代理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十四-参考链接"><span class="nav-text">十四. 参考链接</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Pyfdtic</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('-1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
